{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4179d5bd",
   "metadata": {},
   "source": [
    "# NMMC Grievance Analytics — Senior DS Notebook\n",
    "\n",
    "**Goal:** Build an end‑to‑end, production‑oriented analysis of NMMC grievances (quality checks → KPI layer → text mining → sub‑topic taxonomy → modeling ideas → deployment/monitoring plan).\n",
    "\n",
    "> **Data source:** `channel-wise-grievance-data.xlsx`\n",
    "\n",
    "## Headline snapshot (from the current file)\n",
    "- **Rows:** 11,103\n",
    "- **Unique grievance IDs:** 11,103\n",
    "- **Created date range:** 2024-10-11 04:21 → 2025-12-31 10:56\n",
    "- **Closed rate (by current status):** 96.39%\n",
    "\n",
    "**Top departments (count):**\n",
    "- Encroachment: 1,932\n",
    "- City Engineer: 1,617\n",
    "- Water Supply: 1,316\n",
    "- Solid Waste Management: 1,211\n",
    "- Electrical: 972\n",
    "\n",
    "**Top wards (count):**\n",
    "- Belapur: 1,746\n",
    "- Koparkhairane: 1,662\n",
    "- Airoli: 1,463\n",
    "- Ghansoli: 1,370\n",
    "- Vashi: 1,283\n",
    "\n",
    "**Top missing fields (% rows):**\n",
    "- Complaint Description: 5.63%\n",
    "- Closing Remark: 4.52%\n",
    "- Mobile No.: 0.26%\n",
    "- Reported by User Name: 0.07%\n",
    "- Complaint Location: 0.02%\n",
    "\n",
    "---\n",
    "\n",
    "### How to use this notebook\n",
    "1. Run cells top‑to‑bottom.\n",
    "2. Replace file path if needed.\n",
    "3. This notebook **masks PII** (mobile numbers + user names) in outputs by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12850e",
   "metadata": {},
   "source": [
    "## 1) Load data + repair header row\n",
    "\n",
    "This Excel has a **two‑row header structure**: a banner row + a row that contains the real column names.\n",
    "We read with `header=1`, then promote the first data row into headers.\n",
    "\n",
    "We also parse `Created Date` into a timestamp column `_created_at`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "SRC_PATH = r\"/mnt/data/channel-wise-grievance-data.xlsx\"\n",
    "\n",
    "raw = pd.read_excel(SRC_PATH, header=1)\n",
    "headers = raw.iloc[0].tolist()\n",
    "\n",
    "df = raw.iloc[1:].copy()\n",
    "df.columns = [str(h).strip() for h in headers]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df[\"_created_at\"] = pd.to_datetime(df[\"Created Date\"].astype(str), dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "df.shape, df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d94d9e",
   "metadata": {},
   "source": [
    "## 2) Data contract & schema checks (senior DS hygiene)\n",
    "\n",
    "We treat the dataset like an **API contract**: verify required columns, validate types, and build fast “tripwires” that fail early.\n",
    "\n",
    "Also: this dataset contains **PII** (names, mobile numbers, locations). We will:\n",
    "- Create masked variants for any notebook display.\n",
    "- Keep raw PII only in memory (no writing back to disk unless explicitly required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_COLS = [\n",
    "    \"Grievance Id\",\n",
    "    \"Created Date\",\n",
    "    \"Reported by User Name\",\n",
    "    \"Mobile No.\",\n",
    "    \"Complaint Location\",\n",
    "    \"Complaint Subject\",\n",
    "    \"Complaint Description\",\n",
    "    \"Current Status\",\n",
    "    \"Current Department Name\",\n",
    "    \"Ward Name\",\n",
    "    \"Current User Name\",\n",
    "    \"Closing Remark\",\n",
    "]\n",
    "\n",
    "missing_cols = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "assert not missing_cols, f\"Missing required columns: {missing_cols}\"\n",
    "\n",
    "# Quick type checks\n",
    "assert df[\"Grievance Id\"].notna().all(), \"Grievance Id has nulls\"\n",
    "assert df[\"_created_at\"].notna().all(), \"Created Date parsing failed for some rows\"\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fe676",
   "metadata": {},
   "source": [
    "## 3) PII‑safe view helpers\n",
    "\n",
    "We mask:\n",
    "- Mobile numbers → keep last 3 digits\n",
    "- User names → initials only\n",
    "\n",
    "This makes sharing screenshots / outputs safer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78237fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def mask_mobile(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = re.sub(r\"\\D+\", \"\", str(x))\n",
    "    if len(s) < 4: return \"****\"\n",
    "    return \"*\"*(len(s)-3) + s[-3:]\n",
    "\n",
    "def mask_name(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    parts = [p for p in re.split(r\"\\s+\", str(x).strip()) if p]\n",
    "    if not parts: return np.nan\n",
    "    # initials: \"Prashant Verat\" -> \"P. V.\"\n",
    "    return \" \".join([p[0].upper() + \".\" for p in parts[:3]])\n",
    "\n",
    "def stable_hash(x, salt=\"nmmc\"):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = f\"{salt}::{str(x)}\".encode(\"utf-8\")\n",
    "    return hashlib.sha256(s).hexdigest()[:12]\n",
    "\n",
    "df_safe = df.copy()\n",
    "df_safe[\"Mobile No.\"] = df_safe[\"Mobile No.\"].apply(mask_mobile)\n",
    "df_safe[\"Reported by User Name\"] = df_safe[\"Reported by User Name\"].apply(mask_name)\n",
    "df_safe[\"Current User Name\"] = df_safe[\"Current User Name\"].apply(mask_name)\n",
    "\n",
    "df_safe.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e998b7",
   "metadata": {},
   "source": [
    "## 4) Data quality report\n",
    "\n",
    "Senior DS checklist:\n",
    "- Missingness & “structural nulls”\n",
    "- Uniqueness of primary keys\n",
    "- Value distributions (categoricals)\n",
    "- Timestamp integrity (range, timezone assumptions)\n",
    "- Text fields: empty strings, extreme lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61719b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary key uniqueness\n",
    "pk_dupes = df[\"Grievance Id\"].duplicated().sum()\n",
    "print(\"Duplicate Grievance Id rows:\", pk_dupes)\n",
    "\n",
    "# Missingness (%)\n",
    "missing = (df.isna().mean()*100).sort_values(ascending=False)\n",
    "missing.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic categorical distributions\n",
    "for col in [\"Current Status\",\"Current Department Name\",\"Ward Name\"]:\n",
    "    display(df[col].value_counts(dropna=False).head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length sanity\n",
    "text_cols = [\"Complaint Subject\",\"Complaint Description\",\"Closing Remark\",\"Complaint Location\"]\n",
    "for c in text_cols:\n",
    "    s = df[c].astype(str).fillna(\"\")\n",
    "    lens = s.str.len()\n",
    "    print(c, \"min/median/p95/max:\", int(lens.min()), int(lens.median()), int(lens.quantile(0.95)), int(lens.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8c2f5",
   "metadata": {},
   "source": [
    "## 5) KPI Layer (operational analytics)\n",
    "\n",
    "Even before AI, you can deliver value with a stable KPI layer:\n",
    "- Ticket volume over time (daily/weekly)\n",
    "- Closed vs open backlog\n",
    "- Escalation & reopen rates\n",
    "- Pareto charts by department/ward\n",
    "- “Backlog age” (time since created) for non‑closed items\n",
    "\n",
    "> Note: the file does **not** contain a resolved/closed timestamp. So resolution‑time SLAs cannot be computed directly.\n",
    "If NMMC can export `Closed Date` / `Last Updated Date`, we can add SLA metrics and survival analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8860ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time buckets\n",
    "df_kpi = df.copy()\n",
    "df_kpi[\"date\"] = df_kpi[\"_created_at\"].dt.date\n",
    "df_kpi[\"week\"] = df_kpi[\"_created_at\"].dt.to_period(\"W\").astype(str)\n",
    "df_kpi[\"month\"] = df_kpi[\"_created_at\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# Daily volume\n",
    "daily = df_kpi.groupby(\"date\")[\"Grievance Id\"].size()\n",
    "\n",
    "plt.figure()\n",
    "daily.plot()\n",
    "plt.title(\"Daily grievance volume\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "daily.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status mix\n",
    "status = df_kpi[\"Current Status\"].value_counts()\n",
    "plt.figure()\n",
    "status.plot(kind=\"bar\")\n",
    "plt.title(\"Current status distribution\")\n",
    "plt.xlabel(\"Status\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "(status / status.sum()).round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee336543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backlog (non-closed) and age buckets\n",
    "asof = df_kpi[\"_created_at\"].max()  # dataset as-of\n",
    "open_mask = df_kpi[\"Current Status\"].astype(str).str.upper().ne(\"CLOSED\")\n",
    "df_open = df_kpi.loc[open_mask].copy()\n",
    "df_open[\"age_days\"] = (asof - df_open[\"_created_at\"]).dt.total_seconds() / 86400.0\n",
    "\n",
    "df_open[\"age_bucket\"] = pd.cut(\n",
    "    df_open[\"age_days\"],\n",
    "    bins=[-1, 1, 3, 7, 14, 30, 60, 120, 99999],\n",
    "    labels=[\"<=1d\",\"1-3d\",\"3-7d\",\"7-14d\",\"14-30d\",\"30-60d\",\"60-120d\",\">120d\"]\n",
    ")\n",
    "\n",
    "display(df_open[\"age_bucket\"].value_counts(dropna=False).sort_index())\n",
    "\n",
    "plt.figure()\n",
    "df_open[\"age_bucket\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Open backlog age buckets (as-of max Created Date)\")\n",
    "plt.xlabel(\"Age bucket\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto: top departments / wards\n",
    "top_dept = df_kpi[\"Current Department Name\"].value_counts().head(15)\n",
    "top_ward = df_kpi[\"Ward Name\"].value_counts()\n",
    "\n",
    "plt.figure()\n",
    "top_dept.plot(kind=\"bar\")\n",
    "plt.title(\"Top 15 departments by volume\")\n",
    "plt.xlabel(\"Department\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "top_ward.plot(kind=\"bar\")\n",
    "plt.title(\"Wards by volume\")\n",
    "plt.xlabel(\"Ward\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a75d48",
   "metadata": {},
   "source": [
    "## 6) Text analytics (fast, explainable)\n",
    "\n",
    "We start with an explainable baseline:\n",
    "- Clean text (lowercase, strip punctuation, normalize whitespace)\n",
    "- TF‑IDF features\n",
    "- Topic discovery via **NMF** (interpretable topics)\n",
    "- Clustering via **KMeans** to propose “sub‑topics”\n",
    "\n",
    "These clusters are **not** final categories; they are an *assist* to:\n",
    "1) propose a taxonomy  \n",
    "2) speed up labeling  \n",
    "3) become training data for a supervised classifier (or LLM prompt + eval set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d362b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # keep letters/numbers and basic punctuation; remove very noisy chars\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return s\n",
    "\n",
    "df_text = df.copy()\n",
    "df_text[\"text\"] = (df_text[\"Complaint Subject\"].fillna(\"\").astype(str) + \" \" +\n",
    "                   df_text[\"Complaint Description\"].fillna(\"\").astype(str)).map(clean_text)\n",
    "\n",
    "df_text[\"text\"].str.len().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ed322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=40000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "X = vectorizer.fit_transform(df_text[\"text\"])\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling: NMF\n",
    "n_topics = 12\n",
    "nmf = NMF(n_components=n_topics, random_state=42, init=\"nndsvda\", max_iter=400)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "def top_terms_for_topic(topic_idx, n=12):\n",
    "    top_idx = np.argsort(H[topic_idx])[::-1][:n]\n",
    "    return feature_names[top_idx].tolist()\n",
    "\n",
    "topics = {f\"topic_{i}\": top_terms_for_topic(i, 14) for i in range(n_topics)}\n",
    "pd.DataFrame({\"topic\": list(topics.keys()), \"top_terms\": list(topics.values())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b66cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster discovery (sub-topic candidates)\n",
    "k = 18\n",
    "km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "clusters = km.fit_predict(W)\n",
    "\n",
    "df_text[\"cluster\"] = clusters\n",
    "\n",
    "# For each cluster, show representative terms by averaging TF-IDF vectors\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def cluster_top_terms(cluster_id, topn=15):\n",
    "    idx = np.where(clusters == cluster_id)[0]\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub = X[idx]\n",
    "    mean_tfidf = np.asarray(sub.mean(axis=0)).ravel()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:topn]\n",
    "    return feature_names[top_idx].tolist()\n",
    "\n",
    "cluster_summary = []\n",
    "for c in range(k):\n",
    "    n = int((df_text[\"cluster\"]==c).sum())\n",
    "    cluster_summary.append({\n",
    "        \"cluster\": c,\n",
    "        \"n\": n,\n",
    "        \"top_terms\": cluster_top_terms(c, 16)\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_summary).sort_values(\"n\", ascending=False)\n",
    "cluster_df.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de24f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples from the largest clusters (PII-safe display)\n",
    "sample = (\n",
    "    df_text.merge(df_safe[[\"Grievance Id\",\"Reported by User Name\",\"Mobile No.\"]], on=\"Grievance Id\", how=\"left\")\n",
    "    .loc[:, [\"Grievance Id\",\"_created_at\",\"Current Department Name\",\"Ward Name\",\"Current Status\",\n",
    "             \"cluster\",\"Reported by User Name\",\"Mobile No.\",\"Complaint Subject\",\"Complaint Description\"]]\n",
    ")\n",
    "\n",
    "for c in cluster_df.head(5)[\"cluster\"]:\n",
    "    display(sample[sample[\"cluster\"]==c].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dbb63",
   "metadata": {},
   "source": [
    "## 7) From clusters → real “Sub‑Topics” (human + LLM loop)\n",
    "\n",
    "A robust taxonomy workflow (practical, senior DS approach):\n",
    "\n",
    "1. **Seed taxonomy** from cluster summaries (above).\n",
    "2. **Human review** (domain team) to merge/split/rename into business‑friendly sub‑topics.\n",
    "3. **Create a gold eval set** (300–800 labeled rows): stratified by department/ward/status and cluster.\n",
    "4. Use **Gemini** (or any LLM) to label sub‑topics with a strict schema:\n",
    "   - `sub_topic`\n",
    "   - `confidence`\n",
    "   - `rationale` (short)\n",
    "   - `needs_human_review` (bool)\n",
    "\n",
    "5. Measure quality on eval set (accuracy/F1 + confusion analysis).  \n",
    "6. Productionize with:\n",
    "   - rules for low‑confidence → human queue\n",
    "   - monitoring drift: top n-grams, embedding drift, volume drift, per-dept error rates\n",
    "\n",
    "Below is a *prompt template* you can use for Gemini to propose sub‑topics consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_SUBTOPIC_PROMPT = '''\n",
    "You are an expert municipal grievance analyst.\n",
    "\n",
    "TASK:\n",
    "Given a grievance record (subject + description + department + ward), assign ONE sub-topic from the allowed list.\n",
    "\n",
    "OUTPUT JSON ONLY (no markdown):\n",
    "{\n",
    "  \"sub_topic\": \"<one of ALLOWED_SUB_TOPICS>\",\n",
    "  \"confidence\": <float 0..1>,\n",
    "  \"needs_human_review\": <true|false>,\n",
    "  \"rationale\": \"<<=25 words>\"\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- Choose exactly ONE sub_topic.\n",
    "- If information is missing/ambiguous OR confidence < 0.65 => needs_human_review = true.\n",
    "- Do NOT output personal data (names, phone numbers).\n",
    "- Prefer business-meaningful buckets (actionable) over vague themes.\n",
    "\n",
    "ALLOWED_SUB_TOPICS:\n",
    "<PASTE_YOUR_FINAL_TAXONOMY_HERE>\n",
    "\n",
    "RECORD:\n",
    "- Department: {department}\n",
    "- Ward: {ward}\n",
    "- Subject: {subject}\n",
    "- Description: {description}\n",
    "'''.strip()\n",
    "\n",
    "print(GEMINI_SUBTOPIC_PROMPT[:700] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a20f4",
   "metadata": {},
   "source": [
    "## 8) Baseline supervised model (department prediction)\n",
    "\n",
    "Why do this even if you use an LLM?\n",
    "- Cheap, fast, stable baseline\n",
    "- Detect outliers + routing errors\n",
    "- Guardrail for LLM (disagreement checks)\n",
    "\n",
    "We train a simple TF‑IDF + Logistic Regression classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d207909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a compact dataset with enough text\n",
    "ml = df_text.copy()\n",
    "ml[\"y_dept\"] = df[\"Current Department Name\"].astype(str)\n",
    "\n",
    "X = vectorizer.fit_transform(ml[\"text\"])\n",
    "y = ml[\"y_dept\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45d9f6",
   "metadata": {},
   "source": [
    "## 9) Productization notes (what “senior DS” looks like)\n",
    "\n",
    "### Minimum viable analytics layer (2–4 weeks)\n",
    "- KPI Cockpit: volume, backlog, reopen/escalation, department/ward Pareto\n",
    "- Cluster-driven sub-topic exploration\n",
    "- Exportable “review queue” for humans (low-confidence / novel clusters)\n",
    "\n",
    "### Minimum viable AI layer (4–8 weeks)\n",
    "- Final sub-topic taxonomy (v1) + eval set\n",
    "- Gemini labeler + human review loop + analytics on label distribution\n",
    "- Monitoring: drift + per-ward/per-dept performance\n",
    "\n",
    "### Data improvements to request from Probity/NMMC\n",
    "To unlock SLA + accountability analytics:\n",
    "- `Last Updated Date`\n",
    "- `Closed Date` (or status history table)\n",
    "- `Channel` (app/web/call center/WhatsApp/etc.)\n",
    "- `Category` (if exists in source system)\n",
    "- `Geo` (lat/long or standardized location codes)\n",
    "- `Assignment history` (who/when it moved between users/depts)\n",
    "\n",
    "### Governance / Privacy\n",
    "- PII minimization in all downstream datasets\n",
    "- Role-based access controls\n",
    "- Hash IDs for join keys where possible\n",
    "- Audit logs for data exports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca7474",
   "metadata": {},
   "source": [
    "## 10) Next steps checklist\n",
    "\n",
    "1. Confirm target output: **Sub‑Topic taxonomy** vs **Department routing** vs both.  \n",
    "2. Add `Closed Date` / status history export (critical for SLA).  \n",
    "3. Create a 500‑row eval set (stratified by cluster + department + ward).  \n",
    "4. Iterate taxonomy v1 → v2 using confusion analysis.  \n",
    "5. Decide production path:\n",
    "   - LLM-only (Gemini) with eval + monitoring\n",
    "   - Hybrid: ML baseline + LLM fallback\n",
    "   - Fully supervised after enough labels\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
